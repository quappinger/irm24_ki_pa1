{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67536517-51a6-4136-a886-27c0f4690ade",
   "metadata": {},
   "source": [
    "# Project Work - IRM24\n",
    "## Jürgen Aumayr & Natalia Trudova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0042f-8b21-4942-bed7-445c4bbe5424",
   "metadata": {},
   "source": [
    "## Vergleich der Modelle\n",
    "\n",
    "### **Precision (Genauigkeit der positiven Vorhersagen)**\n",
    "Precision misst den Anteil der tatsächlich korrekten positiven Vorhersagen an allen als positiv vorhergesagten Fällen.\n",
    "\n",
    "Formel:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    " \n",
    "- **TP (True Positives):** Korrekt als positiv klassifizierte Fälle\n",
    "- **FP (False Positives):** Fälschlicherweise als positiv klassifizierte Fälle\n",
    "\n",
    "Eine hohe Precision bedeutet, dass das Modell selten fälschlicherweise positive Vorhersagen trifft. Das ist besonders wichtig, wenn die Kosten für Fehlalarme (False Positives) hoch sind.\n",
    "\n",
    "\n",
    "### **Recall (Sensitivität, Trefferquote)**\n",
    "Recall misst den Anteil der korrekt erkannten positiven Fälle an allen tatsächlich positiven Fällen.\n",
    "\n",
    "Formel:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **TP (True Positives):** Korrekt als positiv klassifizierte Fälle\n",
    "- **FN (False Negatives):** Tatsächlich positive Fälle, die das Modell nicht erkannt hat\n",
    "\n",
    "Ein hoher Recall bedeutet, dass das Modell die meisten tatsächlichen Positiven findet. Das ist wichtig, wenn das Verpassen eines positiven Falls (False Negative) schwerwiegende Folgen hat.\n",
    "\n",
    "\n",
    "### **F1-Score (Harmonisches Mittel von Precision und Recall)**\n",
    "Der F1-Score kombiniert Precision und Recall zu einer einzigen Metrik, indem er ihr harmonisches Mittel berechnet.\n",
    "\n",
    "Formel:\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Der F1-Score ist besonders nützlich bei unausgeglichenen Datensätzen, da er nur dann hoch ist, wenn sowohl Precision als auch Recall hoch sind. Er ist die Standardmetrik, wenn ein ausgewogenes Verhältnis zwischen Precision und Recall wichtig ist.\n",
    "\n",
    "\n",
    "### **Support (Anzahl der wahren Instanzen pro Klasse)**\n",
    "Support gibt an, wie viele tatsächliche Beispiele es pro Klasse im Datensatz gibt.\n",
    "\n",
    "Support selbst ist keine Leistungsmetrik, sondern gibt Kontext: Ein hoher oder niedriger Support kann die Aussagekraft von Precision, Recall und F1-Score beeinflussen, insbesondere bei stark unbalancierten Klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c6166b-451e-4ecc-885f-06b1d5e455fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Notwendige Bibliotheken importieren \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Optimierte Datentypen definieren\n",
    "dtypes = {\n",
    "    'PassengerId': 'int32',  # 32-bit Integer für IDs ist ausreichend\n",
    "    'Survived': 'int8',      # 8-bit Integer für binäre Werte (0/1)\n",
    "    'Pclass': 'int8',        # 8-bit Integer für kleine Kategorien (1-3)\n",
    "    'Name': 'str',           # String für Namen\n",
    "    'Sex': 'category',       # Kategorie-Typ für höhere Effizienz bei wiederholten Werten\n",
    "    'Age': 'float32',        # 32-bit Float statt Standard 64-bit\n",
    "    'SibSp': 'int8',         # 8-bit Integer für kleine Zahlen\n",
    "    'Parch': 'int8',         # 8-bit Integer für kleine Zahlen\n",
    "    'Ticket': 'str',         # String für Ticketnummern\n",
    "    'Fare': 'float32',       # 32-bit Float für Fahrpreise\n",
    "    'Cabin': 'str',          # String für Kabinennummern\n",
    "    'Embarked': 'category'   # Kategorie-Typ für Einschiffungshäfen (S, C, Q)\n",
    "}\n",
    "\n",
    "# Daten mit optimierten Datentypen einlesen \n",
    "train_df = pd.read_csv('data/train.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401f9e12-9913-4742-b114-fa79c6fa99e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def preprocess_features(df):\n",
    "    \"\"\"\n",
    "    Bereitet die Titanic-Daten für das Modelltraining vor und\n",
    "    erzeugt optimierte Features aus den Rohdaten.\n",
    "    \"\"\"\n",
    "    # Kopie erstellen, um Originaldaten nicht zu verändern\n",
    "    dataset = df.copy()\n",
    "    \n",
    "    # Geschlecht in numerisch konvertieren\n",
    "    dataset['Sex'] = dataset['Sex'].map({'female': 0, 'male': 1})\n",
    "    \n",
    "    # Fehlende Alterswerte mit Median füllen - nach Klasse und Geschlecht gruppiert\n",
    "    age_median = dataset.groupby(['Pclass', 'Sex'])['Age'].transform('median')\n",
    "    dataset['Age'] = dataset['Age'].fillna(age_median)\n",
    "\n",
    "    # NaN-Werte durch den Median ersetzen\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n",
    "    \n",
    "    # Alter in Kategorien einteilen\n",
    "    dataset['Age'] = pd.cut(\n",
    "        dataset['Age'], \n",
    "        bins=[0, 18, 60, np.inf], \n",
    "        labels=[1, 2, 3]\n",
    "    ).astype('int8')\n",
    "    \n",
    "    # Fahrpreise in Quartile einteilen für bessere Vergleichbarkeit\n",
    "    dataset['Fare'] = pd.qcut(\n",
    "        dataset['Fare'], \n",
    "        q=4, \n",
    "        labels=[0, 1, 2, 3],\n",
    "        duplicates='drop'  # Verhindert Fehler bei doppelten Grenzwerten\n",
    "    ).astype('int8')\n",
    "    \n",
    "    # Einschiffungshafen in numerische Werte umwandeln\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(dataset['Embarked'].mode()[0])\n",
    "    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype('int8')\n",
    "    \n",
    "    # Binäres Merkmal: Hat der Passagier eine Kabinennummer oder nicht?\n",
    "    dataset['Has_Cabin'] = (dataset['Cabin'].notna()).astype('int8')\n",
    "    \n",
    "    # Berechnung der Familiengröße und Alleinreisenden-Status\n",
    "    dataset['FamilySize'] = (dataset['SibSp'] + dataset['Parch'] + 1).astype('int8')\n",
    "    dataset['IsAlone'] = (dataset['FamilySize'] == 1).astype('int8')\n",
    "    \n",
    "    # Titel aus Namen extrahieren mit regulärem Ausdruck - effizienter als Split\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Titel-Mapping für Kategorisierung\n",
    "    title_mapping = {\n",
    "        \"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5\n",
    "    }\n",
    "    \n",
    "    # Seltenere Titel zusammenfassen, um Overfitting zu vermeiden\n",
    "    rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n",
    "                   'Major', 'Rev', 'Sir', 'Jonkheer', 'Mlle', 'Mme', 'Ms']\n",
    "    dataset['Title'] = dataset['Title'].replace(rare_titles, 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping).fillna(5).astype('int8')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Daten vorverarbeiten und relevante Features auswählen\n",
    "processed_df = preprocess_features(train_df)\n",
    "features_df = processed_df[['Survived', 'Pclass', 'Sex', 'Age', 'Parch', \n",
    "                            'Fare', 'Embarked', 'Has_Cabin', 'FamilySize', \n",
    "                            'IsAlone', 'Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6df68a0-0eb9-44b7-ba03-afaba0eca949",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Features und Zielvariable trennen\n",
    "X = features_df.drop('Survived', axis=1)\n",
    "y = features_df['Survived']\n",
    "\n",
    "# Daten in Trainings- und Validierungssets aufteilen mit stratifizierter Stichprobe\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c274918d-2ee1-48d6-b6d8-f9d7c8e08ffd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 4, 5, 6],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Training\n",
    "# Hyperparameter-Grid für automatische Optimierung\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Parallelisierte Suche mit 5-facher Kreuzvalidierung\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,  # Nutzt alle verfügbaren CPU-Kerne\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Modell trainieren\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b133a8b4-c433-4b13-99e8-f4ca55558166",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6], 'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [100, 200]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forests Training\n",
    "# Hyperparameter-Grid für Random Forest\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "\n",
    "# GridSearchCV für Hyperparameter-Tuning verwenden\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Modell trainieren\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f58093-d5bb-4f71-b4b6-bb16068f9550",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(n_iter_no_change=5,\n",
       "                                                  random_state=42,\n",
       "                                                  subsample=0.8,\n",
       "                                                  validation_fraction=0.2),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.05, 0.1], 'max_depth': [3, 5],\n",
       "                         'min_samples_split': [10, 20],\n",
       "                         'n_estimators': [100, 200]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Training\n",
    "\n",
    "# Gradient Boosting Parameter-Space\n",
    "gb_params = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [10, 20]\n",
    "}\n",
    "\n",
    "# Grid Search mit Early Stopping\n",
    "gb_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(\n",
    "        subsample=0.8,\n",
    "        validation_fraction=0.2,\n",
    "        n_iter_no_change=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    param_grid=gb_params,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7de16ed1-c9ff-455e-a373-e96aed39ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Decision Tree---------------------\n",
      "Validierungs-Genauigkeit: 0.7933\n",
      "\n",
      "Klassifikationsbericht:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Gestorben       0.80      0.88      0.84       110\n",
      "    Überlebt       0.78      0.65      0.71        69\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.77      0.77       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "-------------------Random Forests---------------------\n",
      "Validierungsgenauigkeit: 0.7989\n",
      "\n",
      "Klassifikationsbericht:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Gestorben       0.81      0.87      0.84       110\n",
      "    Überlebt       0.77      0.68      0.72        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.78      0.78       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "------------------Gradient Boosting-------------------\n",
      "Validierungsgenauigkeit: 0.8101\n",
      "\n",
      "Klassifikationsbericht:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Gestorben       0.81      0.91      0.85       110\n",
      "    Überlebt       0.82      0.65      0.73        69\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.78      0.79       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparision\n",
    "\n",
    "# Decision Tree\n",
    "best_model = grid_search.best_estimator_\n",
    "val_pred = best_model.predict(X_val)\n",
    "val_acc_dt = accuracy_score(y_val, val_pred)\n",
    "print(\"--------------------Decision Tree---------------------\")\n",
    "print(f\"Validierungs-Genauigkeit: {val_acc_dt:.4f}\")\n",
    "print(\"\\nKlassifikationsbericht:\")\n",
    "print(classification_report(y_val, val_pred, target_names=['Gestorben', 'Überlebt']))\n",
    "\n",
    "# Random Forests\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "val_predictions = best_rf_model.predict(X_val)\n",
    "val_acc_rf = accuracy_score(y_val, val_predictions)\n",
    "print(\"-------------------Random Forests---------------------\")\n",
    "print(f\"Validierungsgenauigkeit: {val_acc_rf:.4f}\")\n",
    "print(\"\\nKlassifikationsbericht:\")\n",
    "print(classification_report(y_val, val_predictions, target_names=['Gestorben', 'Überlebt']))\n",
    "\n",
    "# Gradient Boosting\n",
    "best_gb = gb_search.best_estimator_\n",
    "val_preds = best_gb.predict(X_val)\n",
    "val_acc_gb = accuracy_score(y_val, val_preds)\n",
    "print(\"------------------Gradient Boosting-------------------\")\n",
    "print(f\"Validierungsgenauigkeit: {accuracy_score(y_val, val_preds):.4f}\")\n",
    "print(\"\\nKlassifikationsbericht:\")\n",
    "print(classification_report(y_val, val_preds, target_names=['Gestorben', 'Überlebt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e973c-2851-4008-b2eb-1d4fc6426e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
